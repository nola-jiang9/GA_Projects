{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Project 3: Web APIs & NLP SubReddit Classification (Walmart & Costco)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive Summary\n",
    "\n",
    "In a most recent survey, Walmart was ranked **LAST** for supermarkets in the 2021 American Customer Satisfaction Index Retail and Consumer Shipping Report, We as a Data Science Team in Walmart are been appointed by Top Management on tasks to understand what users(Customers & Walmart employees) think of Walmart and our competitors, to understand where positive feedback will continue to be reinforced and adopted while negative feedback can be addressed and prevented, with the leverage of Data Science approach, We will suggest on what actions We could take and eventually improve the social media image of Walmart as well as improve Customer Satisfaction.\n",
    "\n",
    "As Social media presence is vital in our day and age, Reddit is an American social news aggregation, web content rating, and discussion website featuring user-posted stories. As of February 2021, Reddit was ranked the 18th most visited website in the world and 7th most visited website in the U.S.\n",
    "\n",
    "As a start, we will be looking into the posts on Walmart's subreddit page, alongside a chosen competitor, **Costco** (who was ranked at top 2 in above suvey mentioned)\n",
    "\n",
    "**Primary Objective**: \n",
    "To enhance our understanding of Walmart's social media image on reddit so as to introduce strategy for improvement.\n",
    "\n",
    "**Secondary Objective**: \n",
    "To identify what subreddit users think of both supermarkets, where positive feedback will continue to be reinforced and adopted while negative feedback can be addressed and prevented.\n",
    "\n",
    "The __Primary stakeholder is:__ Walmart Corporate, and the __Secondary Stakeholder is:__ Walmart Consumers.\n",
    "\n",
    "The success of this project will be evaluated based on a classification model having a higher prediction accuracy and F1 Score than the baseline score of 50%.\n",
    "\n",
    "The classification model through Natural language processing will aid us in \"understanding\" the contents of the subreddits.\n",
    "Posts from Walmart and Costco subreddits are scraped, analyzed, cleaned, processed and ran through several classification models (e.g. Logistic Regression, KNearestNeighbor, Multinomial Naive Bayes, Random Tree Classifier).\n",
    "The final production model that we chose was __Logistic Regression with TfidfVectorizer__, at a classification prediction rate of __93.3%__.\n",
    "\n",
    "- Reference: \n",
    "    - https://www.supermarketnews.com/issues-trends/customer-satisfaction-fell-supermarkets-2020\n",
    "    - https://www.barrons.com/articles/walmart-stock-is-falling-on-earnings-miss-muted-outlook-51613657909\n",
    "    - https://www.vox.com/recode/22423706/walmart-memo-retail-amazon-target-instacart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Project notebook organisation:<br>\n",
    "**1 - SubReddit Web Scrapping** (current notebook)<br>\n",
    "[2 - Exploratory Data Analysis and Preprocessing](./2_exploratory_data_analysis_and_preprocessing.ipynb)<br>\n",
    "[3 - Classification Model and Recommendation](./3_Classification_Model_and_Recommendation.ipynb)<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: SubReddit Web Scrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents:\n",
    "- [1. Import Libraries](#1.-Import-Libraries)\n",
    "- [2. Web Posts Scrapping](#2.-Web-Posts-Scrapping)\n",
    "\n",
    "In Part 1, the main task is doing posts Data Scrapping from the 2 subreddits--Walmart & Costco (around 1k posts been scrapped for each subreddit). And data has been merged into 1 dataframe and exported for next Part 2-EDA & Data-Processing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. Import Libraries\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "# !pip install psaw, for data scrapping\n",
    "from psaw import PushshiftAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Web Posts Scrapping\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Define Public Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url =\"https://api.pushshift.io/reddit/search/submission\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the function to do data scrapping from subreddits\n",
    "def subreddit_scrapping(subreddit, n_iter):\n",
    "    \n",
    "    # setting up an empty post list, inital after set to none\n",
    "    df_posts = []\n",
    "    \n",
    "    # Use EpochConverter to get CurrentTimeStamp in UTC integer\n",
    "    current_time= 1623345408\n",
    "    # Sunday, June 6, 2021 12:33:23 AM\n",
    "    \n",
    "    score_range=\">100\"\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        \n",
    "        print(f'Scrapping {i+1}th 100 posts in progress...')\n",
    "        \n",
    "        res = requests.get(\n",
    "            url,\n",
    "            params = {\n",
    "                \"subreddit\":subreddit,\n",
    "                \"size\":100,   #every request retrieve 100 posts\n",
    "                \"is_self\":True, # only scrap text posts\n",
    "                \"sort_type\": \"created_utc\",\n",
    "                \"sort\":\"desc\",\n",
    "                \"score\":\">10\",\n",
    "                \"before\":current_time\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        if res.status_code == 200:\n",
    "            # if request calls success\n",
    "            time.sleep(random.random()*5)  # random sleep time for next request\n",
    "            df = pd.DataFrame(res.json()['data'])\n",
    "            \n",
    "            # there are many columns but not really relevant to this NLP project\n",
    "            # I will only use these useful fields\n",
    "            df = df.loc[:,['subreddit',\"created_utc\",\"id\",\"title\",\"selftext\",\"is_self\",\"upvote_ratio\",\"score\", \"num_comments\"]]\n",
    "            df_posts.append(df)\n",
    "            \n",
    "            current_time = df.created_utc.min()\n",
    "             \n",
    "        else:\n",
    "            print(res.status_code)\n",
    "            break\n",
    "            \n",
    "    print('Scapping all posts completed!')\n",
    "    \n",
    "    return pd.concat(df_posts,axis=0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "\n",
    "I have tried to scrap Walmart & Costco Data using above defined function, but noticed that the posts' score are at the range of 0 to 8 (without setting the score range parmeter), but after set the score parameter (score >10), I cannot get close 1k posts.\n",
    "\n",
    "Since Our business objective is trying to analyze and model on most hot posts, I will use another method to pull the posts by using Pushshift API functions directly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Walmart & Costco Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = PushshiftAPI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1  Scrapping Walmart data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To scrap posts in recent 6 months since 2021-06-01 & only collect text posts \n",
    "api_request_generator = api.search_submissions(subreddit='walmart', \n",
    "                                               before = 1623489389, \n",
    "                                               after = 1590981341, \n",
    "                                               is_self = True, \n",
    "                                               limit =1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 33.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "aita_submissions = pd.DataFrame([submission.d_ for submission in api_request_generator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1490, 75)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We are mainly focus on the text posts of the title & selftext\n",
    "# So to remove duplicate rows on these 2 fields\n",
    "df_walmart=aita_submissions.drop_duplicates(subset = ['title', 'selftext'],keep = 'last').reset_index(drop = True)\n",
    "df_walmart.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['all_awardings', 'allow_live_comments', 'author',\n",
       "       'author_flair_background_color', 'author_flair_css_class',\n",
       "       'author_flair_richtext', 'author_flair_text', 'author_flair_text_color',\n",
       "       'author_flair_type', 'author_fullname', 'author_patreon_flair',\n",
       "       'author_premium', 'awarders', 'can_mod_post', 'contest_mode',\n",
       "       'created_utc', 'domain', 'full_link', 'gildings', 'id',\n",
       "       'is_created_from_ads_ui', 'is_crosspostable', 'is_meta',\n",
       "       'is_original_content', 'is_reddit_media_domain', 'is_robot_indexable',\n",
       "       'is_self', 'is_video', 'link_flair_background_color',\n",
       "       'link_flair_richtext', 'link_flair_text_color', 'link_flair_type',\n",
       "       'locked', 'media_only', 'no_follow', 'num_comments', 'num_crossposts',\n",
       "       'over_18', 'parent_whitelist_status', 'permalink', 'pinned', 'pwls',\n",
       "       'retrieved_on', 'score', 'selftext', 'send_replies', 'spoiler',\n",
       "       'stickied', 'subreddit', 'subreddit_id', 'subreddit_subscribers',\n",
       "       'subreddit_type', 'suggested_sort', 'thumbnail', 'title',\n",
       "       'total_awards_received', 'treatment_tags', 'upvote_ratio', 'url',\n",
       "       'whitelist_status', 'wls', 'created', 'removed_by_category',\n",
       "       'author_flair_template_id', 'link_flair_template_id', 'link_flair_text',\n",
       "       'post_hint', 'preview', 'banned_by', 'edited', 'media_metadata',\n",
       "       'thumbnail_height', 'thumbnail_width', 'author_cakeday', 'poll_data'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_walmart.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are many columns but not really relevant to this NLP project\n",
    "# I will only use these useful fields\n",
    "df_walmart = df_walmart[['subreddit', 'created_utc', 'id', 'title', 'selftext',\n",
    "       'upvote_ratio', 'score', 'num_comments']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1490, 8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_walmart.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>walmart</td>\n",
       "      <td>1623481278</td>\n",
       "      <td>ny0t2k</td>\n",
       "      <td>The whole meat wall, in one night?</td>\n",
       "      <td>Is that even possible?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>walmart</td>\n",
       "      <td>1623479940</td>\n",
       "      <td>ny0hk4</td>\n",
       "      <td>Early Lunches</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>walmart</td>\n",
       "      <td>1623477525</td>\n",
       "      <td>nxzwvb</td>\n",
       "      <td>Cap 2/Overnight Team leads</td>\n",
       "      <td>Due to unforseen circumstances, I was not able...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>walmart</td>\n",
       "      <td>1623476491</td>\n",
       "      <td>nxzndf</td>\n",
       "      <td>Finally promoted MYSELF to customer</td>\n",
       "      <td>This is going to be a long one so buckle up, f...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>walmart</td>\n",
       "      <td>1623476227</td>\n",
       "      <td>nxzkvz</td>\n",
       "      <td>Pointing out after putting in your two week no...</td>\n",
       "      <td>So I submitted my 2 week notice yesterday and ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit  created_utc      id  \\\n",
       "0   walmart   1623481278  ny0t2k   \n",
       "1   walmart   1623479940  ny0hk4   \n",
       "2   walmart   1623477525  nxzwvb   \n",
       "3   walmart   1623476491  nxzndf   \n",
       "4   walmart   1623476227  nxzkvz   \n",
       "\n",
       "                                               title  \\\n",
       "0                 The whole meat wall, in one night?   \n",
       "1                                      Early Lunches   \n",
       "2                         Cap 2/Overnight Team leads   \n",
       "3                Finally promoted MYSELF to customer   \n",
       "4  Pointing out after putting in your two week no...   \n",
       "\n",
       "                                            selftext  upvote_ratio  score  \\\n",
       "0                             Is that even possible?           1.0      1   \n",
       "1                                          [removed]           1.0      1   \n",
       "2  Due to unforseen circumstances, I was not able...           1.0      1   \n",
       "3  This is going to be a long one so buckle up, f...           1.0      1   \n",
       "4  So I submitted my 2 week notice yesterday and ...           1.0      1   \n",
       "\n",
       "   num_comments  \n",
       "0            15  \n",
       "1             0  \n",
       "2            14  \n",
       "3             9  \n",
       "4            11  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_walmart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Walmart posts are from 2021-05-29 01:59:22 to 2021-06-12 15:01:18\n"
     ]
    }
   ],
   "source": [
    "min_posts_datetime = datetime.datetime.fromtimestamp(df_walmart['created_utc'].min())\n",
    "max_posts_datetime =datetime.datetime.fromtimestamp(df_walmart['created_utc'].max())\n",
    "\n",
    "print (f'The Walmart posts are from {min_posts_datetime} to {max_posts_datetime}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "I've removed duplicated rows (duplicate \"title\" & \"selftext\"), and able to get 1490 posts for Walmart SubReddit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Scrap Costco Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To scrap posts in recent 6 months since 2021-01-01 & only collect text posts \n",
    "api_request_generator = api.search_submissions(subreddit='costco', \n",
    "                                               before = 1623489389, \n",
    "                                               after = 1592069466, \n",
    "                                               is_self = True, \n",
    "                                               limit =1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 33.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "aita_submissions = pd.DataFrame([submission.d_ for submission in api_request_generator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1497, 74)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We are mainly focus on the text posts of the title & selftext\n",
    "# So to remove duplicate rows on these 2 fields\n",
    "df_costco=aita_submissions.drop_duplicates(subset = ['title', 'selftext'],keep = 'last').reset_index(drop = True)\n",
    "df_costco.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "\n",
    "Same as Walmart posts, I removed duplicated rows (duplicate \"title\" & \"selftext\") and able to get 1497 posts for Costco SubReddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are many columns but not really relevant to this NLP project\n",
    "# I will only use these useful fields\n",
    "\n",
    "df_costco = df_costco[['subreddit', 'created_utc', 'id', 'title', 'selftext',\n",
    "       'upvote_ratio', 'score', 'num_comments']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['subreddit', 'created_utc', 'id', 'title', 'selftext', 'upvote_ratio',\n",
       "       'score', 'num_comments'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_costco.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1497, 8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_costco.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Costco</td>\n",
       "      <td>1623484046</td>\n",
       "      <td>ny1g1y</td>\n",
       "      <td>Teenage girl harassed by a male employee</td>\n",
       "      <td>I was too shaken up to talk to a manager while...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Costco</td>\n",
       "      <td>1623468009</td>\n",
       "      <td>nxxc5b</td>\n",
       "      <td>Does Costco sell hanging baskets of flowers? R...</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Costco</td>\n",
       "      <td>1623467264</td>\n",
       "      <td>nxx4b3</td>\n",
       "      <td>18 year anniversary</td>\n",
       "      <td>Would be my 18th year anniversary had i stayed...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Costco</td>\n",
       "      <td>1623463893</td>\n",
       "      <td>nxw4nm</td>\n",
       "      <td>PLEASE include the Costco location (City/State...</td>\n",
       "      <td>I asked once \"Where did you find this item?\" -...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Costco</td>\n",
       "      <td>1623462565</td>\n",
       "      <td>nxvqeg</td>\n",
       "      <td>Add burgers to food court!</td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit  created_utc      id  \\\n",
       "0    Costco   1623484046  ny1g1y   \n",
       "1    Costco   1623468009  nxxc5b   \n",
       "2    Costco   1623467264  nxx4b3   \n",
       "3    Costco   1623463893  nxw4nm   \n",
       "4    Costco   1623462565  nxvqeg   \n",
       "\n",
       "                                               title  \\\n",
       "0           Teenage girl harassed by a male employee   \n",
       "1  Does Costco sell hanging baskets of flowers? R...   \n",
       "2                                18 year anniversary   \n",
       "3  PLEASE include the Costco location (City/State...   \n",
       "4                         Add burgers to food court!   \n",
       "\n",
       "                                            selftext  upvote_ratio  score  \\\n",
       "0  I was too shaken up to talk to a manager while...           1.0      1   \n",
       "1                                          [removed]           1.0      1   \n",
       "2  Would be my 18th year anniversary had i stayed...           1.0      1   \n",
       "3  I asked once \"Where did you find this item?\" -...           1.0      1   \n",
       "4                                                              1.0      1   \n",
       "\n",
       "   num_comments  \n",
       "0            24  \n",
       "1             2  \n",
       "2             1  \n",
       "3             2  \n",
       "4            28  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_costco.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Costco posts are from 2021-04-08 09:23:11 to 2021-06-12 15:47:26\n"
     ]
    }
   ],
   "source": [
    "min_posts_datetime = datetime.datetime.fromtimestamp(df_costco['created_utc'].min())\n",
    "max_posts_datetime =datetime.datetime.fromtimestamp(df_costco['created_utc'].max())\n",
    "\n",
    "print (f'The Costco posts are from {min_posts_datetime} to {max_posts_datetime}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commentsï¼š\n",
    "\n",
    "We can see about 15 days' scrapped Walmart posts have the same amount as about 2+ months Costco posts. In other words, more people are cared about Walmart than Costco, as Walmart is more popular than Costco. \n",
    "\n",
    "Also this might because more diversity business in Walmart, mainly including Walmart U.S.,Walmart Supercenter, Walmart Discount Store, Walmart Neighborhood Market, Walmart Express, Walmart International and Sam's Club, in total 7 segmentations, besides Walmart is also doing Charity and contribute to COVID-19 (coronavirus) Vaccination. More business and operating models, more comments from the customers or employees. \n",
    "\n",
    "From the research, with over 2.3 million employees worldwide, Walmart has faced a torrent of lawsuits and issues with regards to its workforce. These issues involve low wages, poor working conditions, inadequate health care, and issues involving the company's strong anti-union policies. From Oct 2020, Walmart has launched new employee structure which there might lead to a lot of discussions and comments as well. \n",
    "\n",
    "We will leverag on the already established Sub-Reddit pages of Costco and Walmart-to extract posts, look at the most-talked about subjects, and to finally explore the key similarities and differences between the 2 sub-reddits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Merge Walmart & Costco Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge = pd.concat([df_walmart, df_costco], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Costco     1497\n",
       "walmart    1490\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the datas were combined correctly \n",
    "df_merge['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "\n",
    "In total, I get 1497 Costco posts and 1490 Walmart posts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Export Data to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../datasets/01_SubReddit_Web_Scrapping/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_walmart.to_csv(data_path + 'walmart_scrapped_posts.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_costco.to_csv(data_path + 'costco_scrapped_posts.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge.to_csv(data_path + 'combined_posts.csv', index= False)\n",
    "\n",
    "# in case of rerun this notebook cause data over-writtern, \n",
    "# the combined posts csv file I've also manually copied to dataset folder \"02_Exploratory_Data_Analysis_and_Preprocessing\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Please go to Notebook 02_Exploratory Data Analysis and Preprocessing.ipynb__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

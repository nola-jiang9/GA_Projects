{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Web APIs & NLP SubReddit Classification (Walmart & Costco)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Project notebook organisation:<br>\n",
    "[1 - SubReddit Web Scrapping](./01_SubReddit_Web_Scrapping.ipynb)<br>\n",
    "[2 - Exploratory Data Analysis and Preprocessing](./2_exploratory_data_analysis_and_preprocessing.ipynb)<br>\n",
    "**3 - Classification Model and Recommendation** (current notebook)<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "# Part 3: Classifcation Model and Recommendation\n",
    "\n",
    "### Contents:\n",
    "- [1. Import Libraries](#1.-Import-Libraries)\n",
    "- [2. Load Datasets](#2.-Load-Datasets)\n",
    "- [3. Modeling](#3.-Modeling)\n",
    "- [4. Model Tuning & Production Model](#4.-Model-Tuning-&-Production-Model)\n",
    "- [5. Sentiment Analysis & Business Summary](#5.-Sentiment-Analysis-&-Business-Summary)\n",
    "- [6. Future Steps](#6.-Future-Steps)\n",
    "\n",
    "In this part, I have attempted several classification models (e.g. Logistic Regression, KNearestNeighbor, Multinomial Naive Bayes, Random Tree Classifier) with the combination of utilizing CountVectorizer or TfidfVectorizer for subreddit classification.\n",
    "\n",
    "I used F1 Score & AUC Score to compare among different model prediction, also it has a higher prediction than the baseline score of __50.5%__.The final production model that I chose was Logistic Regression with TfidfVectorizer, at a classification prediction rate of __93.3%__.\n",
    "\n",
    "Lastly, I did Sentiment Analysis to aid for a better business recommendation to achieve our primary and scecondary objectives.\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. Import Libraries\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# from wordcloud import WordCloud\n",
    "from textblob import TextBlob\n",
    "import spacy #spacy stopwords\n",
    "from spacy.lang.en import English\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS,TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import datetime\n",
    "import string\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Load Datasets\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cleaned corpus-result from part 2-EDA & Pre-processing\n",
    "data_path = \"../datasets/02_Exploratory_Data_Analysis_and_Preprocessing/\"\n",
    "df= pd.read_csv(data_path  + 'cleaned_posts_corpus.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2881, 13)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>post_created_date</th>\n",
       "      <th>text</th>\n",
       "      <th>text_length</th>\n",
       "      <th>text_word_count</th>\n",
       "      <th>subreddit_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>walmart</td>\n",
       "      <td>1623481278</td>\n",
       "      <td>ny0t2k</td>\n",
       "      <td>the whole meat wall one night</td>\n",
       "      <td>that even possible</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>2021-06-12 15:01:18</td>\n",
       "      <td>the whole meat wall one night that even possible</td>\n",
       "      <td>51</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>walmart</td>\n",
       "      <td>1623477525</td>\n",
       "      <td>nxzwvb</td>\n",
       "      <td>cap overnight team leads</td>\n",
       "      <td>due unforseen circumstances was not able apply...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>2021-06-12 13:58:45</td>\n",
       "      <td>cap overnight team lead due unforseen circumst...</td>\n",
       "      <td>494</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>walmart</td>\n",
       "      <td>1623476227</td>\n",
       "      <td>nxzkvz</td>\n",
       "      <td>pointing out after putting your two week notice</td>\n",
       "      <td>submitted week notice yesterday and points so...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>2021-06-12 13:37:07</td>\n",
       "      <td>pointing out after putting your two week notic...</td>\n",
       "      <td>233</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit  created_utc      id  \\\n",
       "0   walmart   1623481278  ny0t2k   \n",
       "1   walmart   1623477525  nxzwvb   \n",
       "2   walmart   1623476227  nxzkvz   \n",
       "\n",
       "                                              title  \\\n",
       "0                    the whole meat wall one night    \n",
       "1                          cap overnight team leads   \n",
       "2  pointing out after putting your two week notice    \n",
       "\n",
       "                                            selftext  upvote_ratio  score  \\\n",
       "0                                that even possible            1.0      1   \n",
       "1  due unforseen circumstances was not able apply...           1.0      1   \n",
       "2   submitted week notice yesterday and points so...           1.0      1   \n",
       "\n",
       "   num_comments    post_created_date  \\\n",
       "0            15  2021-06-12 15:01:18   \n",
       "1            14  2021-06-12 13:58:45   \n",
       "2            11  2021-06-12 13:37:07   \n",
       "\n",
       "                                                text  text_length  \\\n",
       "0  the whole meat wall one night that even possible            51   \n",
       "1  cap overnight team lead due unforseen circumst...          494   \n",
       "2  pointing out after putting your two week notic...          233   \n",
       "\n",
       "   text_word_count  subreddit_type  \n",
       "0                9               0  \n",
       "1               82               0  \n",
       "2               36               0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>subreddit_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the whole meat wall one night that even possible</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cap overnight team lead due unforseen circumst...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pointing out after putting your two week notic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>they drug test just got job local walmart truc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>there way limit hour availability online basic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  subreddit_type\n",
       "0  the whole meat wall one night that even possible                0\n",
       "1  cap overnight team lead due unforseen circumst...               0\n",
       "2  pointing out after putting your two week notic...               0\n",
       "3  they drug test just got job local walmart truc...               0\n",
       "4  there way limit hour availability online basic...               0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We are only used 2 columns for Classification Model\n",
    "df_model = df[['text', 'subreddit_type']]\n",
    "df_model.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modeling\n",
    "\n",
    "In this section, I will explore 4 models. They are Naive Bayes, Random Forest, Logistic Regression and KNN. I will run each model twice, first time using CountVertorizer and second time using Term Frequency-Inverse Document Frequency (TF-IDF). I will provide a short explaination of each model and evaluate its performance success/downfalls.\n",
    "\n",
    "__Naive Bayes Classifier__:\n",
    "- it is a family of probabilistic algorithms based on applying Bayes’ theorem with the “naive” assumption of conditional independence between every pair of a feature.\n",
    "- Bayes theorem calculates probability P(c|x) where c is the class of the possible outcomes and x is the given instance which has to be classified, representing some certain features.P(c|x) = P(x|c) * P(c) / P(x)\n",
    "- Naive Bayes are mostly used in natural language processing (NLP) problems. Naive Bayes predict the tag of a text. They calculate the probability of each tag for a given text and then output the tag with the highest one.\n",
    "- The advantages of Naive Bayes are it is easy to calculate probabilities and returns empirically accurate result.\n",
    "- The disadvantage is the assumption of feature independence is unrealistic, especially in the case of text data. The predicted probabilites could be not so good.\n",
    "\n",
    "__The Random Forest (RF) classifiers__\n",
    "- It is suitable for dealing with the high dimensional noisy data in text classification. An RF model comprises a set of decision trees each of which is trained using random subsets of features. \n",
    "- It requires much computational power as well as resources as it builds numerous trees to combine their outputs. \n",
    "- It also requires much time for training as it combines a lot of decision trees to determine the class.\n",
    "- Due to the ensemble of decision trees, it also suffers interpretability and fails to determine the significance of each variable.\n",
    "\n",
    "We also attempted it and see how well it for our subreddit classification.\n",
    "\n",
    "__Logistic Regression__\n",
    "- It is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables.\n",
    "- Logistic regression uses the logit link to bend our line of best fit. This allows us to predict between 0 and 1 for any value of inputs. \n",
    "    \n",
    "#### $$\\text{logit}\\left(P(Y = 1)\\right) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\cdots + \\beta_pX_p$$ ####\n",
    "\n",
    "- The advantage of logistic regression are the coefficients are interpretable and it shares similar properties to linear regression. \n",
    "- The disadvantage is the assumption of linearity between the dependent variable and independent variables. Logistic Regression requires average or no mulitcollinearity between independent variables. \n",
    "\n",
    "__K-Nearest-Neighbor__\n",
    "\n",
    "- The k-nearest neighbors algorithm (k-NN) is a non-parametric method. In k-NN classification, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.\n",
    "- The advantage of KNN is it has no training period. KNN is called Lazy Learner (Instance based learning). It does not learn anything in the training period. It does not derive any discriminative function from the training data. In other words, there is no training period for it. It stores the training dataset and learns from it only at the time of making real time predictions. This makes the KNN algorithm much faster than other algorithms that require training e.g. SVM, Linear Regression etc. \n",
    "- Since the KNN algorithm requires no training before making predictions, new data can be added seamlessly which will not impact the accuracy of the algorithm. \n",
    "- KNN is very easy to implement. There are only two parameters required to implement KNN i.e. the value of K and the distance function (e.g. Euclidean or Manhattan etc.)\n",
    "- The disadvantage of KNN is it does not work well with large dataset. In large datasets, the cost of calculating the distance between the new point and each existing points is huge which degrades the performance of the algorithm.\n",
    "- The KNN algorithm doesn't work well with high dimensional data because with large number of dimensions, it becomes difficult for the algorithm to calculate the distance in each dimension.\n",
    "- There is also no parameters for interpretation of coefficient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Create Model Feature & Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_model['text']\n",
    "y = df_model['subreddit_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                   y,\n",
    "                                                   random_state = 42,\n",
    "                                                   test_size=0.33,\n",
    "                                                   shuffle = True,\n",
    "                                                   stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "\n",
    "A train test split will be utilized to train the model and see how the model performs with 'unseen' (test) inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.50538\n",
       "0    0.49462\n",
       "Name: subreddit_type, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "\n",
    "The baseline accuracy is the percentage of the majority class-Costco which is 50.5%. It serves as the benchmark for my model to beat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Model Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confusion matrix is a table that is often used to describe the performance of a classification model (or \"classifier\") on a set of test data for which the true values are known, and by looking at the ROC curve and we can determine whether our ROC curve is good or not by looking at AUC (Area Under the Curve). \n",
    "\n",
    "These are 4 parameters for a confusion matrix: True Positives (TP),True Negatives (TN),False Positives (FP) & False Negatives (FN).False positives and false negatives, these values occur when your actual class contradicts with the predicted class.\n",
    "With these 4 parameters We can calculate Accuracy, Precision, Recall and F1 score.\n",
    "\n",
    "Accuracy= TP+TN/TP+FP+FN+TN - Accuracy is the most intuitive performance measure and it is simply a ratio of correctly predicted observation to the total observations. One may think that, if we have high accuracy then our model is best. Yes, accuracy is a great measure but only when you have symmetric datasets where values of false positive and false negatives are almost same. Therefore, you have to look at other parameters to evaluate the performance of your model. \n",
    "\n",
    "Precision=TP/TP+FP - Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. The question that this metric answer is of all posts that labeled as Walmart or Costco, how many actually corrected labeled? High precision relates to the low false positive rate. \n",
    "\n",
    "Recall (Sensitivity)= TP/TP+FN - Recall is the ratio of correctly predicted positive observations to the all observations in actual class.\n",
    "\n",
    "F1 score - F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. Intuitively it is not as easy to understand as accuracy, but F1 is usually more useful than accuracy, especially if you have an uneven class distribution. Accuracy works best if false positives and false negatives have similar cost. If the cost of false positives and false negatives are very different, it’s better to look at both Precision and Recall. \n",
    "\n",
    "**I will use F1 Score as my different model evaluations metric**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as what I used in Exploratory Data Analysis portion, I used the same Stopwords list with the combination of stopwords from 3 different libraries: nltk, spacy & sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stopwords():\n",
    "    nltk_stopwords = set(stopwords.words('english'))\n",
    "    spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "    sklearn_stopwords = set(ENGLISH_STOP_WORDS)\n",
    "\n",
    "    all_stopwords = set()\n",
    "    all_stopwords |= spacy_stopwords\n",
    "    all_stopwords |= nltk_stopwords\n",
    "    all_stopwords |= sklearn_stopwords\n",
    "\n",
    "    return all_stopwords \n",
    "\n",
    "all_stopwords = get_stopwords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update stopwords as they repeated with high frequency in the text but not much meaning \n",
    "all_stopwords.update(['www', 'costco', 'wa', 'doe','ha','know',\n",
    "                      'told','feel','like','tikok','com', 'webp', 'amp', 'html', 'her','walmart', \"wal\",\"mart\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stopwords = list(all_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 CountVectorize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The count vectorizer has been used before in the exploratory data analysis portion because it examines a corpus, which is a collection of documents, and returns a vectorized array of the most prominent words and the count of those words accross each document. It is a necessary step in order to create an array for classification, because we cannot simply stick in words of text into classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.1 Navie Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer(token_pattern=r'[a-zA-Z]{2,}', analyzer = \"word\", stop_words=all_stopwords)),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "pipe_params = {\n",
    "    'cvec__max_features': [2_000, 3_000, 4_000, 5_000],\n",
    "    'cvec__min_df': [2, 3, 4, 6],\n",
    "    'cvec__max_df': [.03, .05, .1,.2, 0.8, 0.85, 0.9],\n",
    "    'cvec__ngram_range': [(1,1),(1,2),(2,3)]\n",
    "}\n",
    "\n",
    "rscv = RandomizedSearchCV(pipe, # what object are we optimizing?\n",
    "                  param_distributions =pipe_params, # what parameters values are we searching?\n",
    "                  cv=5) # 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[('cvec',\n",
       "                                              CountVectorizer(stop_words=['but',\n",
       "                                                                          'alone',\n",
       "                                                                          'down',\n",
       "                                                                          'ca',\n",
       "                                                                          'hence',\n",
       "                                                                          'know',\n",
       "                                                                          'all',\n",
       "                                                                          'none',\n",
       "                                                                          'seem',\n",
       "                                                                          'can',\n",
       "                                                                          '‘d',\n",
       "                                                                          \"she's\",\n",
       "                                                                          'sometimes',\n",
       "                                                                          'un',\n",
       "                                                                          \"'ve\",\n",
       "                                                                          'thence',\n",
       "                                                                          'didn',\n",
       "                                                                          'co',\n",
       "                                                                          'latter',\n",
       "                                                                          'still',\n",
       "                                                                          'something',\n",
       "                                                                          'else',\n",
       "                                                                          'thus',\n",
       "                                                                          'into',\n",
       "                                                                          'became',\n",
       "                                                                          'moreover',\n",
       "                                                                          'make',\n",
       "                                                                          'thereafter',\n",
       "                                                                          'around',\n",
       "                                                                          'less', ...],\n",
       "                                                              token_pattern='[a-zA-Z]{2,}')),\n",
       "                                             ('nb', MultinomialNB())]),\n",
       "                   param_distributions={'cvec__max_df': [0.03, 0.05, 0.1, 0.2,\n",
       "                                                         0.8, 0.85, 0.9],\n",
       "                                        'cvec__max_features': [2000, 3000, 4000,\n",
       "                                                               5000],\n",
       "                                        'cvec__min_df': [2, 3, 4, 6],\n",
       "                                        'cvec__ngram_range': [(1, 1), (1, 2),\n",
       "                                                              (2, 3)]})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit GridSearch to training data.\n",
    "rscv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB with CountVectorize-train score: 0.869\n",
      "MultinomialNB with CountVectorize-test score: 0.858\n",
      "MultinomialNB by RandomizedGridSearchCV best params: {'cvec__ngram_range': (1, 1), 'cvec__min_df': 2, 'cvec__max_features': 4000, 'cvec__max_df': 0.1}\n"
     ]
    }
   ],
   "source": [
    "print(f'MultinomialNB with CountVectorize-train score: {round(rscv.best_score_,3)}')\n",
    "print(f'MultinomialNB with CountVectorize-test score: {round(rscv.score(X_test, y_test),3)}')\n",
    "print(f'MultinomialNB by RandomizedGridSearchCV best params: {rscv.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           precision    recall  f1-score     support\n",
      "Predict 0   0.831683  0.893617  0.861538  470.000000\n",
      "Predict 1   0.887892  0.823285  0.854369  481.000000\n",
      "accuracy    0.858044  0.858044  0.858044    0.858044\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test,rscv.best_estimator_.predict(X_test), target_names=['Predict 0', 'Predict 1'], output_dict=True)\n",
    "class_table = pd.DataFrame(report).transpose().drop(['macro avg','weighted avg'],axis=0)\n",
    "print(class_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "\n",
    "- The model fits quite well, has a high mean accuracy score of 0.869 and 0.858 for test score.\n",
    "- The model also has a higher recall-0.89 for Costco. (the % of positive cases model been labeled)\n",
    "- The model has a higher precision-0.88 for Walmart. (the % of the model prediction is correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer(token_pattern=r'[a-zA-Z]{2,}', analyzer = \"word\", stop_words=all_stopwords)),\n",
    "    ('rf', RandomForestClassifier(random_state = 42))\n",
    "])\n",
    "\n",
    "pipe_params = {\n",
    "    'rf__n_estimators':[100, 150, 200],\n",
    "    'rf__max_depth': [None, 50, 100],\n",
    "    'rf__max_features': ['auto', 10, 20, 50],\n",
    "    'rf__min_samples_split':[2, 25, 40, 50, 70],\n",
    "    'rf__min_samples_leaf': [1],\n",
    "    'cvec__max_features': [2_000, 3_000, 4_000, 5_000],\n",
    "    'cvec__min_df': [2, 3, 4, 6],\n",
    "    'cvec__max_df': [.03, .05, .1,.2, 0.8, 0.85, 0.9],\n",
    "    'cvec__ngram_range': [(1,1),(1,2),(2,3)]\n",
    "}\n",
    "\n",
    "rscv = RandomizedSearchCV(pipe, # what object are we optimizing?\n",
    "                  param_distributions=pipe_params, # what parameters values are we searching?\n",
    "                  n_iter = 50,\n",
    "                  cv=5) # 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[('cvec',\n",
       "                                              CountVectorizer(stop_words=['but',\n",
       "                                                                          'alone',\n",
       "                                                                          'down',\n",
       "                                                                          'ca',\n",
       "                                                                          'hence',\n",
       "                                                                          'know',\n",
       "                                                                          'all',\n",
       "                                                                          'none',\n",
       "                                                                          'seem',\n",
       "                                                                          'can',\n",
       "                                                                          '‘d',\n",
       "                                                                          \"she's\",\n",
       "                                                                          'sometimes',\n",
       "                                                                          'un',\n",
       "                                                                          \"'ve\",\n",
       "                                                                          'thence',\n",
       "                                                                          'didn',\n",
       "                                                                          'co',\n",
       "                                                                          'latter',\n",
       "                                                                          'still',\n",
       "                                                                          'something',\n",
       "                                                                          'else',\n",
       "                                                                          'thus',\n",
       "                                                                          'into',\n",
       "                                                                          'became',\n",
       "                                                                          'moreover',\n",
       "                                                                          'make',\n",
       "                                                                          'thereafter',\n",
       "                                                                          'around',\n",
       "                                                                          'less', ...],\n",
       "                                                              token_pattern='[a-zA-Z]{2,}')...\n",
       "                   n_iter=50,\n",
       "                   param_distributions={'cvec__max_df': [0.03, 0.05, 0.1, 0.2,\n",
       "                                                         0.8, 0.85, 0.9],\n",
       "                                        'cvec__max_features': [2000, 3000, 4000,\n",
       "                                                               5000],\n",
       "                                        'cvec__min_df': [2, 3, 4, 6],\n",
       "                                        'cvec__ngram_range': [(1, 1), (1, 2),\n",
       "                                                              (2, 3)],\n",
       "                                        'rf__max_depth': [None, 50, 100],\n",
       "                                        'rf__max_features': ['auto', 10, 20,\n",
       "                                                             50],\n",
       "                                        'rf__min_samples_leaf': [1],\n",
       "                                        'rf__min_samples_split': [2, 25, 40, 50,\n",
       "                                                                  70],\n",
       "                                        'rf__n_estimators': [100, 150, 200]})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rscv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest with CountVectorize-train score: 0.837\n",
      "RandomForest with CountVectorize-test score: 0.842\n",
      "RandomForest by RandomizedGridSearchCV best params: {'rf__n_estimators': 200, 'rf__min_samples_split': 50, 'rf__min_samples_leaf': 1, 'rf__max_features': 10, 'rf__max_depth': 100, 'cvec__ngram_range': (1, 2), 'cvec__min_df': 4, 'cvec__max_features': 4000, 'cvec__max_df': 0.85}\n"
     ]
    }
   ],
   "source": [
    "print(f'RandomForest with CountVectorize-train score: {round(rscv.best_score_,3)}')\n",
    "print(f'RandomForest with CountVectorize-test score: {round(rscv.score(X_test, y_test),3)}')\n",
    "print(f'RandomForest by RandomizedGridSearchCV best params: {rscv.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           precision    recall  f1-score     support\n",
      "Predict 0   0.840426  0.840426  0.840426  470.000000\n",
      "Predict 1   0.844075  0.844075  0.844075  481.000000\n",
      "accuracy    0.842271  0.842271  0.842271    0.842271\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test,rscv.best_estimator_.predict(X_test), target_names=['Predict 0', 'Predict 1'], output_dict=True)\n",
    "class_table = pd.DataFrame(report).transpose().drop(['macro avg','weighted avg'],axis=0)\n",
    "print(class_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "\n",
    "- The model fits quite well, has a high mean accuracy score of 0.837 and 0.842 for test score. But it's slightly lower than MultinomialNB model.\n",
    "- The model also has a higher recall-0.844 for Walmart. (the % of positive cases model been labeled)\n",
    "- The model has a higher precision-0.844 for Walmart. (the % of the model prediction is correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.3 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer(token_pattern=r'[a-zA-Z]{2,}', analyzer = \"word\", stop_words=all_stopwords)),\n",
    "    ('lr', LogisticRegression(solver='lbfgs')) # For multiclass problems, only ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ handle multinomial loss\n",
    "])\n",
    "\n",
    "pipe_params = {\n",
    "    'cvec__max_features': [1_000,2_000, 3_000, 4_000, 5_000],\n",
    "    'cvec__min_df': [2, 4, 6],\n",
    "    'cvec__max_df': [.8, .85,.9],\n",
    "    'cvec__ngram_range': [(1,1),(1,2),(2,3)]\n",
    "}\n",
    "\n",
    "rscv = RandomizedSearchCV(pipe, # what object are we optimizing?\n",
    "                  param_distributions =pipe_params, # what parameters values are we searching?\n",
    "                  cv=5, # 5-fold cross-validation.\n",
    "                  n_jobs=-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[('cvec',\n",
       "                                              CountVectorizer(stop_words=['but',\n",
       "                                                                          'alone',\n",
       "                                                                          'down',\n",
       "                                                                          'ca',\n",
       "                                                                          'hence',\n",
       "                                                                          'know',\n",
       "                                                                          'all',\n",
       "                                                                          'none',\n",
       "                                                                          'seem',\n",
       "                                                                          'can',\n",
       "                                                                          '‘d',\n",
       "                                                                          \"she's\",\n",
       "                                                                          'sometimes',\n",
       "                                                                          'un',\n",
       "                                                                          \"'ve\",\n",
       "                                                                          'thence',\n",
       "                                                                          'didn',\n",
       "                                                                          'co',\n",
       "                                                                          'latter',\n",
       "                                                                          'still',\n",
       "                                                                          'something',\n",
       "                                                                          'else',\n",
       "                                                                          'thus',\n",
       "                                                                          'into',\n",
       "                                                                          'became',\n",
       "                                                                          'moreover',\n",
       "                                                                          'make',\n",
       "                                                                          'thereafter',\n",
       "                                                                          'around',\n",
       "                                                                          'less', ...],\n",
       "                                                              token_pattern='[a-zA-Z]{2,}')),\n",
       "                                             ('lr', LogisticRegression())]),\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'cvec__max_df': [0.8, 0.85, 0.9],\n",
       "                                        'cvec__max_features': [1000, 2000, 3000,\n",
       "                                                               4000, 5000],\n",
       "                                        'cvec__min_df': [2, 4, 6],\n",
       "                                        'cvec__ngram_range': [(1, 1), (1, 2),\n",
       "                                                              (2, 3)]})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rscv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticsRegression with CountVectorize-train score: 0.848\n",
      "LogisticsRegression with CountVectorize-test score: 0.825\n",
      "LogisticsRegression by RandomizedGridSearchCV best params: {'cvec__ngram_range': (1, 1), 'cvec__min_df': 2, 'cvec__max_features': 4000, 'cvec__max_df': 0.9}\n"
     ]
    }
   ],
   "source": [
    "print(f'LogisticsRegression with CountVectorize-train score: {round(rscv.best_score_,3)}')\n",
    "print(f'LogisticsRegression with CountVectorize-test score: {round(rscv.score(X_test, y_test),3)}')\n",
    "print(f'LogisticsRegression by RandomizedGridSearchCV best params: {rscv.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           precision    recall  f1-score     support\n",
      "Predict 0   0.816667  0.834043  0.825263  470.000000\n",
      "Predict 1   0.834395  0.817048  0.825630  481.000000\n",
      "accuracy    0.825447  0.825447  0.825447    0.825447\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test,rscv.best_estimator_.predict(X_test), target_names=['Predict 0', 'Predict 1'], output_dict=True)\n",
    "class_table = pd.DataFrame(report).transpose().drop(['macro avg','weighted avg'],axis=0)\n",
    "print(class_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "\n",
    "- The model fits quite well, has a high mean accuracy score of 0.848 and 0.824 for test score. But it's slightly lower than MultinomialNB model, and similar to Random Forest model.\n",
    "- The model also has a higher recall-0.83 for Costco. (the % of positive cases model been labeled)\n",
    "- The model has a higher precision-0.83 for Walmart. (the % of the model prediction is correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.4 KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer(token_pattern=r'[a-zA-Z]{2,}', analyzer = \"word\", stop_words=all_stopwords)),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "pipe_params = {\n",
    "    'cvec__max_features': [1_000,2_000, 3_000, 4_000, 5_000],\n",
    "    'cvec__min_df': [2, 4, 6],\n",
    "    'cvec__max_df': [.5, .6, .7, .8],\n",
    "    'cvec__ngram_range': [(1,1),(1,2),(2,3)],\n",
    "    'knn__n_neighbors': [10, 25],\n",
    "    'knn__weights':['uniform','distance']\n",
    "}\n",
    "\n",
    "rscv = RandomizedSearchCV(pipe, # what object are we optimizing?\n",
    "                  param_distributions =pipe_params, # what parameters values are we searching?\n",
    "                  cv=5, # 5-fold cross-validation.\n",
    "                  n_jobs=-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[('cvec',\n",
       "                                              CountVectorizer(stop_words=['but',\n",
       "                                                                          'alone',\n",
       "                                                                          'down',\n",
       "                                                                          'ca',\n",
       "                                                                          'hence',\n",
       "                                                                          'know',\n",
       "                                                                          'all',\n",
       "                                                                          'none',\n",
       "                                                                          'seem',\n",
       "                                                                          'can',\n",
       "                                                                          '‘d',\n",
       "                                                                          \"she's\",\n",
       "                                                                          'sometimes',\n",
       "                                                                          'un',\n",
       "                                                                          \"'ve\",\n",
       "                                                                          'thence',\n",
       "                                                                          'didn',\n",
       "                                                                          'co',\n",
       "                                                                          'latter',\n",
       "                                                                          'still',\n",
       "                                                                          'something',\n",
       "                                                                          'else',\n",
       "                                                                          'thus',\n",
       "                                                                          'into',\n",
       "                                                                          'became',\n",
       "                                                                          'moreover',\n",
       "                                                                          'make',\n",
       "                                                                          'thereafter',\n",
       "                                                                          'around',\n",
       "                                                                          'less', ...],\n",
       "                                                              token_pattern='[a-zA-Z]{2,}')),\n",
       "                                             ('knn', KNeighborsClassifier())]),\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'cvec__max_df': [0.5, 0.6, 0.7, 0.8],\n",
       "                                        'cvec__max_features': [1000, 2000, 3000,\n",
       "                                                               4000, 5000],\n",
       "                                        'cvec__min_df': [2, 4, 6],\n",
       "                                        'cvec__ngram_range': [(1, 1), (1, 2),\n",
       "                                                              (2, 3)],\n",
       "                                        'knn__n_neighbors': [10, 25],\n",
       "                                        'knn__weights': ['uniform',\n",
       "                                                         'distance']})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rscv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN with CountVectorize-train score: 0.701\n",
      "KNN with CountVectorize-test score: 0.723\n",
      "KNN by RandomizedGridSearchCV best params: {'knn__weights': 'uniform', 'knn__n_neighbors': 10, 'cvec__ngram_range': (1, 2), 'cvec__min_df': 2, 'cvec__max_features': 2000, 'cvec__max_df': 0.5}\n"
     ]
    }
   ],
   "source": [
    "print(f'KNN with CountVectorize-train score: {round(rscv.best_score_,3)}')\n",
    "print(f'KNN with CountVectorize-test score: {round(rscv.score(X_test, y_test),3)}')\n",
    "print(f'KNN by RandomizedGridSearchCV best params: {rscv.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           precision    recall  f1-score     support\n",
      "Predict 0   0.763359  0.638298  0.695249  470.000000\n",
      "Predict 1   0.695341  0.806653  0.746872  481.000000\n",
      "accuracy    0.723449  0.723449  0.723449    0.723449\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test,rscv.best_estimator_.predict(X_test), target_names=['Predict 0', 'Predict 1'], output_dict=True)\n",
    "class_table = pd.DataFrame(report).transpose().drop(['macro avg','weighted avg'],axis=0)\n",
    "print(class_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "\n",
    "- The model fits very bad, seems there's high bias and low variance been captured, it has a low mean accuracy score of 0.701 and 0.723 for test score. It's the lowest score model among of all 4 models. (as all other models fitted quite well, I will assume it's not the data issue)\n",
    "- The model also has a higher recall-0.8 for Walmart. (the % of positive cases model been labeled)\n",
    "- The model has a higher precision-0.76 for Costco. (the % of the model prediction is correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Summary with Count Vectorizer:\n",
    "\n",
    "Summary of 4 model predictions:\n",
    "\n",
    "|Model|Train Score|Test Score|Accuracy|F1 Score|\n",
    "|---|---|---|---|---|\n",
    "|Navie Bayes          |0.869      |0.858|0.858|0.861\n",
    "|Random Forest        |0.837      |0.842|0.842|0.844\n",
    "|Logistic Regression  |0.848      |0.825|0.825|0.825\n",
    "|KNN |0.702|0.723|0.723|0.74\n",
    "\n",
    "- We can see that MultinomialNB model performed the best among these 4 models. It has the highest mean accuracy score (0.858).\n",
    "- From the time spent for GridSearch, Logistic Regression and KNN spent the least time while Random Forest spent the most time and required a higher computing power if there are large dataset.\n",
    "- It's quite surprised KNN didnot perform so well that it has only 0.711 mean accuracy score, it might because of difficult to calculate the distance between each dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6  Term Frequency-Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.1 Navie Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(token_pattern=r'[a-zA-Z]{2,}', analyzer = \"word\", stop_words=all_stopwords)),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "pipe_params = {\n",
    "    'tfidf__max_features': [1_000,2_000, 3_000, 4_000, 5_000],\n",
    "    'tfidf__min_df': [2, 4, 6],\n",
    "    'tfidf__max_df': [.03, .05, .1,.2],\n",
    "    'tfidf__ngram_range': [(1,1),(1,2),(2,3)]\n",
    "}\n",
    "\n",
    "rscv = RandomizedSearchCV(pipe, # what object are we optimizing?\n",
    "                  param_distributions =pipe_params, # what parameters values are we searching?\n",
    "                  cv=5) # 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[('tfidf',\n",
       "                                              TfidfVectorizer(stop_words=['but',\n",
       "                                                                          'alone',\n",
       "                                                                          'down',\n",
       "                                                                          'ca',\n",
       "                                                                          'hence',\n",
       "                                                                          'know',\n",
       "                                                                          'all',\n",
       "                                                                          'none',\n",
       "                                                                          'seem',\n",
       "                                                                          'can',\n",
       "                                                                          '‘d',\n",
       "                                                                          \"she's\",\n",
       "                                                                          'sometimes',\n",
       "                                                                          'un',\n",
       "                                                                          \"'ve\",\n",
       "                                                                          'thence',\n",
       "                                                                          'didn',\n",
       "                                                                          'co',\n",
       "                                                                          'latter',\n",
       "                                                                          'still',\n",
       "                                                                          'something',\n",
       "                                                                          'else',\n",
       "                                                                          'thus',\n",
       "                                                                          'into',\n",
       "                                                                          'became',\n",
       "                                                                          'moreover',\n",
       "                                                                          'make',\n",
       "                                                                          'thereafter',\n",
       "                                                                          'around',\n",
       "                                                                          'less', ...],\n",
       "                                                              token_pattern='[a-zA-Z]{2,}')),\n",
       "                                             ('nb', MultinomialNB())]),\n",
       "                   param_distributions={'tfidf__max_df': [0.03, 0.05, 0.1, 0.2],\n",
       "                                        'tfidf__max_features': [1000, 2000,\n",
       "                                                                3000, 4000,\n",
       "                                                                5000],\n",
       "                                        'tfidf__min_df': [2, 4, 6],\n",
       "                                        'tfidf__ngram_range': [(1, 1), (1, 2),\n",
       "                                                               (2, 3)]})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit GridSearch to training data.\n",
    "rscv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB with Ffidfvectorizer-train score: 0.866\n",
      "MultinomialNB with Ffidfvectorizer-test score: 0.858\n",
      "MultinomialNB by RandomizedGridSearchCV best params: {'tfidf__ngram_range': (1, 2), 'tfidf__min_df': 2, 'tfidf__max_features': 5000, 'tfidf__max_df': 0.2}\n"
     ]
    }
   ],
   "source": [
    "print(f'MultinomialNB with Ffidfvectorizer-train score: {round(rscv.best_score_,3)}')\n",
    "print(f'MultinomialNB with Ffidfvectorizer-test score: {round(rscv.score(X_test, y_test),3)}')\n",
    "print(f'MultinomialNB by RandomizedGridSearchCV best params: {rscv.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           precision    recall  f1-score     support\n",
      "Predict 0   0.834331  0.889362  0.860968  470.000000\n",
      "Predict 1   0.884444  0.827443  0.854995  481.000000\n",
      "accuracy    0.858044  0.858044  0.858044    0.858044\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test,rscv.best_estimator_.predict(X_test), target_names=['Predict 0', 'Predict 1'], output_dict=True)\n",
    "class_table = pd.DataFrame(report).transpose().drop(['macro avg','weighted avg'],axis=0)\n",
    "print(class_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "\n",
    "- The model fits quite well, has a high mean accuracy score of 0.866 and 0.858 for test score. It has a slightly better test score compared to same model with CountVectorizer. \n",
    "- The model also has a higher recall-0.88 for Costco. (the % of positive cases model been labeled)\n",
    "- The model has a higher precision-0.88 for Walmart. (the % of the model prediction is correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(token_pattern=r'[a-zA-Z]{2,}', analyzer = \"word\", stop_words=all_stopwords)),\n",
    "    ('rf', RandomForestClassifier(random_state = 42))\n",
    "])\n",
    "\n",
    "pipe_params = {\n",
    "    'rf__n_estimators':[100, 150, 200],\n",
    "    'rf__max_depth': [None, 50, 100],\n",
    "    'rf__max_features': ['auto', 10, 20, 50],\n",
    "    'rf__min_samples_split':[2, 25, 40, 50, 70],\n",
    "    'rf__min_samples_leaf': [1],\n",
    "    'tfidf__max_features': [1_000, 2_000, 3_000, 4_000, 5_000],\n",
    "    'tfidf__min_df': [2, 3, 4, 6],\n",
    "    'tfidf__max_df': [.03, .05, .1,.2, 0.8, 0.85, 0.9],\n",
    "    'tfidf__ngram_range': [(1,1),(1,2),(2,3)]\n",
    "}\n",
    "\n",
    "rscv = RandomizedSearchCV(pipe, # what object are we optimizing?\n",
    "                  param_distributions=pipe_params, # what parameters values are we searching?\n",
    "                  n_iter = 50,\n",
    "                  cv=5) # 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[('tfidf',\n",
       "                                              TfidfVectorizer(stop_words=['but',\n",
       "                                                                          'alone',\n",
       "                                                                          'down',\n",
       "                                                                          'ca',\n",
       "                                                                          'hence',\n",
       "                                                                          'know',\n",
       "                                                                          'all',\n",
       "                                                                          'none',\n",
       "                                                                          'seem',\n",
       "                                                                          'can',\n",
       "                                                                          '‘d',\n",
       "                                                                          \"she's\",\n",
       "                                                                          'sometimes',\n",
       "                                                                          'un',\n",
       "                                                                          \"'ve\",\n",
       "                                                                          'thence',\n",
       "                                                                          'didn',\n",
       "                                                                          'co',\n",
       "                                                                          'latter',\n",
       "                                                                          'still',\n",
       "                                                                          'something',\n",
       "                                                                          'else',\n",
       "                                                                          'thus',\n",
       "                                                                          'into',\n",
       "                                                                          'became',\n",
       "                                                                          'moreover',\n",
       "                                                                          'make',\n",
       "                                                                          'thereafter',\n",
       "                                                                          'around',\n",
       "                                                                          'less', ...],\n",
       "                                                              token_pattern='[a-zA-Z]{2,}'...\n",
       "                   param_distributions={'rf__max_depth': [None, 50, 100],\n",
       "                                        'rf__max_features': ['auto', 10, 20,\n",
       "                                                             50],\n",
       "                                        'rf__min_samples_leaf': [1],\n",
       "                                        'rf__min_samples_split': [2, 25, 40, 50,\n",
       "                                                                  70],\n",
       "                                        'rf__n_estimators': [100, 150, 200],\n",
       "                                        'tfidf__max_df': [0.03, 0.05, 0.1, 0.2,\n",
       "                                                          0.8, 0.85, 0.9],\n",
       "                                        'tfidf__max_features': [1000, 2000,\n",
       "                                                                3000, 4000,\n",
       "                                                                5000],\n",
       "                                        'tfidf__min_df': [2, 3, 4, 6],\n",
       "                                        'tfidf__ngram_range': [(1, 1), (1, 2),\n",
       "                                                               (2, 3)]})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit GridSearch to training data.\n",
    "rscv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest with Ffidfvectorizer-train score: 0.842\n",
      "RandomForest with Ffidfvectorizer-test score: 0.825\n",
      "RandomForest by RandomizedGridSearchCV best params: {'tfidf__ngram_range': (1, 2), 'tfidf__min_df': 2, 'tfidf__max_features': 4000, 'tfidf__max_df': 0.8, 'rf__n_estimators': 150, 'rf__min_samples_split': 25, 'rf__min_samples_leaf': 1, 'rf__max_features': 10, 'rf__max_depth': 100}\n"
     ]
    }
   ],
   "source": [
    "print(f'RandomForest with Ffidfvectorizer-train score: {round(rscv.best_score_,3)}')\n",
    "print(f'RandomForest with Ffidfvectorizer-test score: {round(rscv.score(X_test, y_test),3)}')\n",
    "print(f'RandomForest by RandomizedGridSearchCV best params: {rscv.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           precision    recall  f1-score     support\n",
      "Predict 0   0.837778  0.802128  0.819565  470.000000\n",
      "Predict 1   0.814371  0.848233  0.830957  481.000000\n",
      "accuracy    0.825447  0.825447  0.825447    0.825447\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test,rscv.best_estimator_.predict(X_test), target_names=['Predict 0', 'Predict 1'], output_dict=True)\n",
    "class_table = pd.DataFrame(report).transpose().drop(['macro avg','weighted avg'],axis=0)\n",
    "print(class_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "\n",
    "- The model fits quite well, has a high mean accuracy score of 0.842 and 0.825 for test score. But it's slightly lower than same model with CountVectorizer.\n",
    "- The model also has a higher recall-0.837 for Walmart. (the % of positive cases model been labeled)\n",
    "- The model has a higher precision-0.848 for Walmart. (the % of the model prediction is correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.3 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(token_pattern=r'[a-zA-Z]{2,}', analyzer = \"word\", stop_words=all_stopwords)),\n",
    "    ('lr', LogisticRegression(solver='lbfgs')) # For multiclass problems, only ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ handle multinomial loss\n",
    "])\n",
    "\n",
    "pipe_params = {\n",
    "    'tfidf__max_features': [1_000,2_000, 3_000, 4_000, 5_000],\n",
    "    'tfidf__min_df': [2,3, 4, 6],\n",
    "    'tfidf__max_df': [.8, .85,.9],\n",
    "    'tfidf__ngram_range': [(1,1),(1,2),(2,3)],\n",
    "}\n",
    "\n",
    "rscv = RandomizedSearchCV(pipe, # what object are we optimizing?\n",
    "                  param_distributions =pipe_params, # what parameters values are we searching?\n",
    "                  cv=5, # 5-fold cross-validation.\n",
    "                  n_jobs=-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[('tfidf',\n",
       "                                              TfidfVectorizer(stop_words=['but',\n",
       "                                                                          'alone',\n",
       "                                                                          'down',\n",
       "                                                                          'ca',\n",
       "                                                                          'hence',\n",
       "                                                                          'know',\n",
       "                                                                          'all',\n",
       "                                                                          'none',\n",
       "                                                                          'seem',\n",
       "                                                                          'can',\n",
       "                                                                          '‘d',\n",
       "                                                                          \"she's\",\n",
       "                                                                          'sometimes',\n",
       "                                                                          'un',\n",
       "                                                                          \"'ve\",\n",
       "                                                                          'thence',\n",
       "                                                                          'didn',\n",
       "                                                                          'co',\n",
       "                                                                          'latter',\n",
       "                                                                          'still',\n",
       "                                                                          'something',\n",
       "                                                                          'else',\n",
       "                                                                          'thus',\n",
       "                                                                          'into',\n",
       "                                                                          'became',\n",
       "                                                                          'moreover',\n",
       "                                                                          'make',\n",
       "                                                                          'thereafter',\n",
       "                                                                          'around',\n",
       "                                                                          'less', ...],\n",
       "                                                              token_pattern='[a-zA-Z]{2,}')),\n",
       "                                             ('lr', LogisticRegression())]),\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'tfidf__max_df': [0.8, 0.85, 0.9],\n",
       "                                        'tfidf__max_features': [1000, 2000,\n",
       "                                                                3000, 4000,\n",
       "                                                                5000],\n",
       "                                        'tfidf__min_df': [2, 3, 4, 6],\n",
       "                                        'tfidf__ngram_range': [(1, 1), (1, 2),\n",
       "                                                               (2, 3)]})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit GridSearch to training data.\n",
    "rscv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticsRegression with Ffidfvectorizer-train score: 0.856\n",
      "LogisticsRegression with Ffidfvectorizer-test score: 0.849\n",
      "LogisticsRegression by RandomizedGridSearchCV best params: {'tfidf__ngram_range': (1, 2), 'tfidf__min_df': 2, 'tfidf__max_features': 2000, 'tfidf__max_df': 0.8}\n"
     ]
    }
   ],
   "source": [
    "print(f'LogisticsRegression with Ffidfvectorizer-train score: {round(rscv.best_score_,3)}')\n",
    "print(f'LogisticsRegression with Ffidfvectorizer-test score: {round(rscv.score(X_test, y_test),3)}')\n",
    "print(f'LogisticsRegression by RandomizedGridSearchCV best params: {rscv.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           precision    recall  f1-score    support\n",
      "Predict 0   0.843882  0.851064  0.847458  470.00000\n",
      "Predict 1   0.853249  0.846154  0.849687  481.00000\n",
      "accuracy    0.848580  0.848580  0.848580    0.84858\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test,rscv.best_estimator_.predict(X_test), target_names=['Predict 0', 'Predict 1'], output_dict=True)\n",
    "class_table = pd.DataFrame(report).transpose().drop(['macro avg','weighted avg'],axis=0)\n",
    "print(class_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "\n",
    "- The model fits quite well, has a high mean accuracy score of 0.856 and 0.849 for test score. But it's slightly lower score compared to same model using Count Vectorizer in unseen test data.\n",
    "- The model also has a higher recall-0.85 for Walmart. (the % of positive cases model been labeled)\n",
    "- The model has a higher precision-0.85 for Walmart. (the % of the model prediction is correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.5 KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(token_pattern=r'[a-zA-Z]{2,}', analyzer = \"word\", stop_words=all_stopwords)),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "pipe_params = {\n",
    "    'tfidf__max_features': [1_000,2_000, 3_000, 4_000, 5_000],\n",
    "    'tfidf__min_df': [2, 3, 4, 6],\n",
    "    'tfidf__max_df': [.5, .6, .7, .8,.95],\n",
    "    'tfidf__ngram_range': [(1,1),(1,2),(2,3)],\n",
    "    'knn__n_neighbors': [5, 6],\n",
    "    'knn__weights':['uniform','distance']\n",
    "}\n",
    "\n",
    "rscv = RandomizedSearchCV(pipe, # what object are we optimizing?\n",
    "                  param_distributions =pipe_params, # what parameters values are we searching?\n",
    "                  cv=5, # 5-fold cross-validation.\n",
    "                  n_jobs=-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[('tfidf',\n",
       "                                              TfidfVectorizer(stop_words=['but',\n",
       "                                                                          'alone',\n",
       "                                                                          'down',\n",
       "                                                                          'ca',\n",
       "                                                                          'hence',\n",
       "                                                                          'know',\n",
       "                                                                          'all',\n",
       "                                                                          'none',\n",
       "                                                                          'seem',\n",
       "                                                                          'can',\n",
       "                                                                          '‘d',\n",
       "                                                                          \"she's\",\n",
       "                                                                          'sometimes',\n",
       "                                                                          'un',\n",
       "                                                                          \"'ve\",\n",
       "                                                                          'thence',\n",
       "                                                                          'didn',\n",
       "                                                                          'co',\n",
       "                                                                          'latter',\n",
       "                                                                          'still',\n",
       "                                                                          'something',\n",
       "                                                                          'else',\n",
       "                                                                          'thus',\n",
       "                                                                          'into',\n",
       "                                                                          'became',\n",
       "                                                                          'moreover',\n",
       "                                                                          'make',\n",
       "                                                                          'thereafter',\n",
       "                                                                          'around',\n",
       "                                                                          'less', ...],\n",
       "                                                              token_pattern='[a-zA-Z]{2,}')),\n",
       "                                             ('knn', KNeighborsClassifier())]),\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'knn__n_neighbors': [5, 6],\n",
       "                                        'knn__weights': ['uniform', 'distance'],\n",
       "                                        'tfidf__max_df': [0.5, 0.6, 0.7, 0.8,\n",
       "                                                          0.95],\n",
       "                                        'tfidf__max_features': [1000, 2000,\n",
       "                                                                3000, 4000,\n",
       "                                                                5000],\n",
       "                                        'tfidf__min_df': [2, 3, 4, 6],\n",
       "                                        'tfidf__ngram_range': [(1, 1), (1, 2),\n",
       "                                                               (2, 3)]})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit GridSearch to training data.\n",
    "rscv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN with Ffidfvectorizer-train score: 0.808\n",
      "KNN with Ffidfvectorizer-test score: 0.797\n",
      "KNN by RandomizedGridSearchCV best params: {'tfidf__ngram_range': (1, 2), 'tfidf__min_df': 3, 'tfidf__max_features': 5000, 'tfidf__max_df': 0.5, 'knn__weights': 'uniform', 'knn__n_neighbors': 6}\n"
     ]
    }
   ],
   "source": [
    "print(f'KNN with Ffidfvectorizer-train score: {round(rscv.best_score_,3)}')\n",
    "print(f'KNN with Ffidfvectorizer-test score: {round(rscv.score(X_test, y_test),3)}')\n",
    "print(f'KNN by RandomizedGridSearchCV best params: {rscv.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           precision    recall  f1-score     support\n",
      "Predict 0   0.747764  0.889362  0.812439  470.000000\n",
      "Predict 1   0.867347  0.706861  0.778923  481.000000\n",
      "accuracy    0.797056  0.797056  0.797056    0.797056\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test,rscv.best_estimator_.predict(X_test), target_names=['Predict 0', 'Predict 1'], output_dict=True)\n",
    "class_table = pd.DataFrame(report).transpose().drop(['macro avg','weighted avg'],axis=0)\n",
    "print(class_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "\n",
    "- The model fits well, it has a slightly lower mean accuracy score of 0.808 and 0.779 for test score. It's the lowest score model among of all 4 models. (as all other models fitted quite well, I will assume it's not the data issue)\n",
    "- The model also has a higher recall-0.88 for Costco. (the % of positive cases model been labeled)\n",
    "- The model has a higher precision-0.86 for Walmart. (the % of the model prediction is correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Summary with TfidF Vectorizer:\n",
    "\n",
    "Summary of 4 model predictions:\n",
    "\n",
    "|Model|Train Score|Test Score|Accuracy|F1 Score|\n",
    "|---|---|---|---|---|\n",
    "|Navie Bayes          |0.866      |0.858|0.858|0.86\n",
    "|Random Forest        |0.842      |0.825|0.825|0.83\n",
    "|Logistic Regression  |0.856      |0.849|0.849|0.85\n",
    "|KNN                  |0.808      |0.797|0.797|0.81\n",
    "\n",
    "- TF-IDF Vectorizer overall improves the model compared to CountVectorizer. (KNN improves a lot while Logistics Regression also has slightly improved)\n",
    "- We can see that both MultinomialNB model & Logistic Regression Model performed the better among these 4 models. They have the highest accuracy Score (0.858) and (0.849).\n",
    "- From the time spent for GridSearch, Logistic Regression and KNN spent the least time while Random Forest spent the most time and required a higher computing power if there are large dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Tuning & Production Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will choose the best model derived from above (Logistic Regression with TfidfVectorizer) to conduct further tuning. \n",
    "\n",
    "This includes a histogram plot to display distribution of both the true and predicted value, and a Receiver Operation Characteristic to illustrate the diagnostic ability of a binary classifier system as its discrimination threshold is varied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the best parameters afte GridSearch\n",
    "tfidf = TfidfVectorizer(token_pattern=r'[a-zA-Z]{2,}',\n",
    "                        analyzer = \"word\",\n",
    "                        stop_words=all_stopwords,\n",
    "                        max_df=0.85,\n",
    "                        min_df=3,\n",
    "                        ngram_range=(1,2),\n",
    "                        max_features=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Predict 0</th>\n",
       "      <td>0.849257</td>\n",
       "      <td>0.851064</td>\n",
       "      <td>0.850159</td>\n",
       "      <td>470.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predict 1</th>\n",
       "      <td>0.854167</td>\n",
       "      <td>0.852391</td>\n",
       "      <td>0.853278</td>\n",
       "      <td>481.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.851735</td>\n",
       "      <td>0.851735</td>\n",
       "      <td>0.851735</td>\n",
       "      <td>0.851735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           precision    recall  f1-score     support\n",
       "Predict 0   0.849257  0.851064  0.850159  470.000000\n",
       "Predict 1   0.854167  0.852391  0.853278  481.000000\n",
       "accuracy    0.851735  0.851735  0.851735    0.851735"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_c = tfidf.fit_transform(X_train)\n",
    "X_test_c = tfidf.transform(X_test)\n",
    "\n",
    "lr = LogisticRegression(solver='lbfgs')\n",
    "lr.fit(X_train_c, y_train)\n",
    "\n",
    "report = classification_report(y_test, lr.predict(X_test_c), target_names=['Predict 0', 'Predict 1'], output_dict=True)\n",
    "class_table = pd.DataFrame(report).transpose().drop(['macro avg','weighted avg'],axis=0)\n",
    "class_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "probs = lr.predict_proba(X_test_c)\n",
    "preds = probs[:,1]\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test, preds)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='GridSearchCV (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model-Logistic Regression + TfidfVectorizer with ROC AUC is: 0.933\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best Model-Logistic Regression + TfidfVectorizer with ROC AUC is: {round(roc_auc,3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf_df_best = pd.DataFrame(X_train_c.todense(), columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get the coefficients df to plot \n",
    "coefficients_df = pd.DataFrame(zip(X_train_tfidf_df_best.columns, np.transpose(lr.coef_)), columns=['features', 'coef'])\n",
    "coefficients_df = coefficients_df.sort_values(by='coef')\n",
    "coefficients_df['coef'] = coefficients_df['coef'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting top 10 and bottom 10 which is best costco and best walmart \n",
    "coefficients_df_plot = pd.concat([coefficients_df[0:10],coefficients_df[-10:]])\n",
    "coefficients_df_plot = coefficients_df_plot.sort_values(by='coef', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot graph \n",
    "plt.figure(figsize=(20,8))\n",
    "sns.barplot(y=coefficients_df_plot['features'], x= coefficients_df_plot['coef'], data=coefficients_df_plot,orient='h',palette = 'coolwarm');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>exp_coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>kirkland</th>\n",
       "      <td>2.419501</td>\n",
       "      <td>11.240253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>membership</th>\n",
       "      <td>2.325137</td>\n",
       "      <td>10.228084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price</th>\n",
       "      <td>2.177972</td>\n",
       "      <td>8.828382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bought</th>\n",
       "      <td>2.156362</td>\n",
       "      <td>8.639646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>warehouse</th>\n",
       "      <td>2.051923</td>\n",
       "      <td>7.782854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sale</th>\n",
       "      <td>1.781042</td>\n",
       "      <td>5.936040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>employee</th>\n",
       "      <td>1.776873</td>\n",
       "      <td>5.911340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chicken</th>\n",
       "      <td>1.694178</td>\n",
       "      <td>5.442168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>member</th>\n",
       "      <td>1.684294</td>\n",
       "      <td>5.388644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tire</th>\n",
       "      <td>1.669667</td>\n",
       "      <td>5.310400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                coef   exp_coef\n",
       "kirkland    2.419501  11.240253\n",
       "membership  2.325137  10.228084\n",
       "price       2.177972   8.828382\n",
       "bought      2.156362   8.639646\n",
       "warehouse   2.051923   7.782854\n",
       "sale        1.781042   5.936040\n",
       "employee    1.776873   5.911340\n",
       "chicken     1.694178   5.442168\n",
       "member      1.684294   5.388644\n",
       "tire        1.669667   5.310400"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_coef_value = pd.DataFrame(data=lr.coef_.T, index=tfidf.get_feature_names())\n",
    "lr_coef_value.columns = ['coef']\n",
    "lr_coef_value['exp_coef'] = np.exp(lr_coef_value['coef'])\n",
    "lr_coef_value.sort_values(by='exp_coef', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justification for picking Logistic Regression TF-IDF & Interpretation of Model\n",
    "\n",
    "1. TF-IDF penalize common words and give rare words more influence. This is relevant to our problem statement, Walmart posts and Costco posts tends to use different terms. For example, Walmart post commonly use the word \"team lead\", \"Covid Vaccine\", \"Covid Shot\" because there are lots of complaints, criticism from various groups and individuals.On the other hand, Costco post might commonly find the word \"food court\" or \"Kirland signature\" because that is a popular of Costco brand and service. Therefore, penalize common words and giving more influence to rare words would help us in our classification problem. \n",
    "\n",
    "2. Comparing the model scores of CountVectorizer and TfidfVectorizer, in all instances models under TfidfVectorizer scores better. \n",
    "\n",
    "3. Why Logistic Regression? Logistic Regression gives a high mean accuracy score for both training data (0.856) and test data (0.849). This means that the model generalises well and scores well on unseen data, though it's not the model with higest score. It also scores much better than the baseline score.  \n",
    " \n",
    "4. Logistic Regression allows me to interpret model coefficients as indicators of feature importance. The interpretation of coefficients allows us to make useful recommendations for our problem statement.  \n",
    "\n",
    "5. Logistic Regression also has some limiations, for instance it assumes independence of independent Variables and the independent variables X1, . . . , Xm are linearly related to the logit of the probability. These are assumations however it may not always be the case. Nonetheless, for our problem statement, logistic regression is an useful model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sentiment Analysis & Business Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Research & Sentiment Analysis\n",
    "\n",
    "From the WordCloud and Top words visualization, also the high coefficients ploted, We are able to identify what subreddit users think of both supermarkets, but it is not sufficient for us to know where positive feedback will continue to be reinforced and adopted while negative feedback can be addressed and prevented.\n",
    "\n",
    "We did Sentiment Analysis in aid for a better business recommendation to answer our problem statement and 2 objectives:\n",
    "\n",
    "**Primary Objective**: \n",
    "To enhance our understanding of Walmart's social media image on reddit so as to introduce strategy for improvement.\n",
    "\n",
    "**Secondary Objective**: \n",
    "To identify what subreddit users think of both supermarkets, where positive feedback will continue to be reinforced and adopted while negative feedback can be addressed and prevented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>post_created_date</th>\n",
       "      <th>text</th>\n",
       "      <th>text_length</th>\n",
       "      <th>text_word_count</th>\n",
       "      <th>subreddit_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>walmart</td>\n",
       "      <td>1623481278</td>\n",
       "      <td>ny0t2k</td>\n",
       "      <td>the whole meat wall one night</td>\n",
       "      <td>that even possible</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>2021-06-12 15:01:18</td>\n",
       "      <td>the whole meat wall one night that even possible</td>\n",
       "      <td>51</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>walmart</td>\n",
       "      <td>1623477525</td>\n",
       "      <td>nxzwvb</td>\n",
       "      <td>cap overnight team leads</td>\n",
       "      <td>due unforseen circumstances was not able apply...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>2021-06-12 13:58:45</td>\n",
       "      <td>cap overnight team lead due unforseen circumst...</td>\n",
       "      <td>494</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>walmart</td>\n",
       "      <td>1623476227</td>\n",
       "      <td>nxzkvz</td>\n",
       "      <td>pointing out after putting your two week notice</td>\n",
       "      <td>submitted week notice yesterday and points so...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>2021-06-12 13:37:07</td>\n",
       "      <td>pointing out after putting your two week notic...</td>\n",
       "      <td>233</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit  created_utc      id  \\\n",
       "0   walmart   1623481278  ny0t2k   \n",
       "1   walmart   1623477525  nxzwvb   \n",
       "2   walmart   1623476227  nxzkvz   \n",
       "\n",
       "                                              title  \\\n",
       "0                    the whole meat wall one night    \n",
       "1                          cap overnight team leads   \n",
       "2  pointing out after putting your two week notice    \n",
       "\n",
       "                                            selftext  upvote_ratio  score  \\\n",
       "0                                that even possible            1.0      1   \n",
       "1  due unforseen circumstances was not able apply...           1.0      1   \n",
       "2   submitted week notice yesterday and points so...           1.0      1   \n",
       "\n",
       "   num_comments    post_created_date  \\\n",
       "0            15  2021-06-12 15:01:18   \n",
       "1            14  2021-06-12 13:58:45   \n",
       "2            11  2021-06-12 13:37:07   \n",
       "\n",
       "                                                text  text_length  \\\n",
       "0  the whole meat wall one night that even possible            51   \n",
       "1  cap overnight team lead due unforseen circumst...          494   \n",
       "2  pointing out after putting your two week notic...          233   \n",
       "\n",
       "   text_word_count  subreddit_type  \n",
       "0                9               0  \n",
       "1               82               0  \n",
       "2               36               0  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol = lambda x: TextBlob(x).sentiment.polarity\n",
    "sub = lambda x: TextBlob(x).sentiment.subjectivity\n",
    "\n",
    "df['polularity'] = df['text'].apply(pol)\n",
    "df['subjectivity'] = df['text'].apply(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating function for calculating positive, negative and neutral\n",
    "# More than 1 --> Positive, equal to 0 --> neutral and less than 0 --> Negative\n",
    "def ratio(x):\n",
    "    if x > 0:\n",
    "        return 1\n",
    "    elif x == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['analysis'] = df['polularity'].apply(ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>text</th>\n",
       "      <th>analysis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>walmart</td>\n",
       "      <td>the whole meat wall one night that even possible</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>walmart</td>\n",
       "      <td>cap overnight team lead due unforseen circumst...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>walmart</td>\n",
       "      <td>pointing out after putting your two week notic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>walmart</td>\n",
       "      <td>they drug test just got job local walmart truc...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>walmart</td>\n",
       "      <td>there way limit hour availability online basic...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit                                               text  analysis\n",
       "0   walmart  the whole meat wall one night that even possible          1\n",
       "1   walmart  cap overnight team lead due unforseen circumst...         1\n",
       "2   walmart  pointing out after putting your two week notic...         0\n",
       "3   walmart  they drug test just got job local walmart truc...        -1\n",
       "4   walmart  there way limit hour availability online basic...        -1"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['subreddit', 'text', 'analysis']][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 438.475x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Sentiment Analysis result group by Subreddit (with count normalization)\n",
    "sns.set(style=\"whitegrid\")\n",
    "#sns.set_palette(\"pastel\")\n",
    "\n",
    "category_order= ['walmart', 'Costco']\n",
    "x,y = 'analysis', 'subreddit'\n",
    "\n",
    "df1 = df.groupby(y)[x].value_counts(normalize=True)\n",
    "df1 = df1.mul(100)\n",
    "df1 = df1.rename('percent').reset_index()\n",
    "\n",
    "g = sns.catplot(x=x,y='percent',hue=y,kind='bar',data=df1, palette=sns.color_palette(['salmon', 'blue']))\n",
    "#g.figsize(5,5)\n",
    "g.ax.set_ylim(0,100)\n",
    "g.fig.suptitle(\"Sentiment Analysis\", y=1.03)\n",
    "\n",
    "for p in g.ax.patches:\n",
    "    txt = str(p.get_height().round(2)) + '%'\n",
    "    txt_x = p.get_x() \n",
    "    txt_y = p.get_height()\n",
    "    g.ax.text(txt_x,txt_y,txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "There are more negative sendiments on Walmart's subreddit posts are double compared to Costco.\n",
    "- For the sentiments of subreddit post of walmart, 19% are neutral, 34% negative & 47% positive.\n",
    "- For the sentiments of subreddit post of costco, 22% are neutral, 23% negative & 54% positive.\n",
    "\n",
    "We would conclude that Walmart's social media image on reddit is not ideal given the numerous posts by Walmart's employees complaining about their workplace environment and conditions. Also supported by the sentiment analysis in which we seek to classify text as having positive or negative emotion.\n",
    "Costco's social media image on the other hand fared well. Costco seems to be able to offer products and services that customers are interested in and had gone on reddit to discuss and give praises about it.\n",
    "\n",
    "Let's take a closer look at what the detailed negative feedback from Walmart posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import top 50 words from part 2- result \n",
    "data_path = \"../datasets/02_Exploratory_Data_Analysis_and_Preprocessing/\"\n",
    "\n",
    "df_walmart_20 = pd.read_csv(data_path + \"walmart_top20_words.csv\")\n",
    "df_costco_20  = pd.read_csv(data_path + \"costco_top20_words.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_walmart_20.columns=['Top_words', \"count_of_words\"]\n",
    "df_costco_20.columns =['Top_words', \"count_of_words\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Top_words</th>\n",
       "      <th>count_of_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>team lead</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>store manager</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>covid leave</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>return work</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>people lead</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>meat produce</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hour shift</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>covid vaccine</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>week ago</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>key date</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>day day</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>job offer</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>month ago</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>promoted customer</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>day work</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>week notice</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>use ppto</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>memorial day</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>background check</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ppto cover</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Top_words  count_of_words\n",
       "0           team lead             162\n",
       "1       store manager              56\n",
       "2         covid leave              49\n",
       "3         return work              27\n",
       "4         people lead              24\n",
       "5        meat produce              22\n",
       "6          hour shift              22\n",
       "7       covid vaccine              21\n",
       "8            week ago              20\n",
       "9            key date              20\n",
       "10            day day              19\n",
       "11          job offer              19\n",
       "12          month ago              18\n",
       "13  promoted customer              18\n",
       "14           day work              17\n",
       "15        week notice              17\n",
       "16           use ppto              17\n",
       "17       memorial day              17\n",
       "18   background check              16\n",
       "19         ppto cover              16"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_walmart_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>analysis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cap overnight team lead due unforseen circumstance wa not able apply for food and consumables originally planned kind glad this point because overnight and cap spot ha opened and staffing ha gotte...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>frustrated and haven been given the raise backpay they promised almost month need some advice there quite bit unpack and talking management ha walked circle start off began transfer right before o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>teaming okay like job when can get around and stick being team lead also involves being over your department and the store whole fine time most day time frustrating walking with plan which get rui...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>moving different area team lead over area and another area open isn that just move management can make why would they want interview already team lead</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>how quit short notice context given post preface this this first job still learning how stuff been working walmart since october but toward the end last month something came and now have move diff...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1415</th>\n",
       "      <td>there way cashier can still give employee their discount without the card started november and still haven gotten discount card told people lead and she just ha reapply for new card every time and...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1420</th>\n",
       "      <td>didn receive myshare bonus wondering anyone can help with advice maybe manager people lead could help got two point and half point the other day that wa incorrectly given call off for vaccine side...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1660</th>\n",
       "      <td>typical get survey specific employee after going the return desk have returned four thing the past month and each time get nice letter from the store manager with self address stamped envelope ask...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1668</th>\n",
       "      <td>super afraid having membership revoked due out portable ac failing and needing returned had terrible luck purchased four portable ac from costco over the last nine year and three them were defecti...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2226</th>\n",
       "      <td>buy clothes item get off brought clothes item get off now have return couple the item ha anybody done this how doe the return work they just refund the cost the the clothes will they longer apply ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>168 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                         text  \\\n",
       "1     cap overnight team lead due unforseen circumstance wa not able apply for food and consumables originally planned kind glad this point because overnight and cap spot ha opened and staffing ha gotte...   \n",
       "5     frustrated and haven been given the raise backpay they promised almost month need some advice there quite bit unpack and talking management ha walked circle start off began transfer right before o...   \n",
       "12    teaming okay like job when can get around and stick being team lead also involves being over your department and the store whole fine time most day time frustrating walking with plan which get rui...   \n",
       "17                                                    moving different area team lead over area and another area open isn that just move management can make why would they want interview already team lead    \n",
       "36    how quit short notice context given post preface this this first job still learning how stuff been working walmart since october but toward the end last month something came and now have move diff...   \n",
       "...                                                                                                                                                                                                       ...   \n",
       "1415  there way cashier can still give employee their discount without the card started november and still haven gotten discount card told people lead and she just ha reapply for new card every time and...   \n",
       "1420  didn receive myshare bonus wondering anyone can help with advice maybe manager people lead could help got two point and half point the other day that wa incorrectly given call off for vaccine side...   \n",
       "1660  typical get survey specific employee after going the return desk have returned four thing the past month and each time get nice letter from the store manager with self address stamped envelope ask...   \n",
       "1668  super afraid having membership revoked due out portable ac failing and needing returned had terrible luck purchased four portable ac from costco over the last nine year and three them were defecti...   \n",
       "2226  buy clothes item get off brought clothes item get off now have return couple the item ha anybody done this how doe the return work they just refund the cost the the clothes will they longer apply ...   \n",
       "\n",
       "      analysis  \n",
       "1            1  \n",
       "5            1  \n",
       "12           1  \n",
       "17           0  \n",
       "36           1  \n",
       "...        ...  \n",
       "1415         1  \n",
       "1420        -1  \n",
       "1660         1  \n",
       "1668        -1  \n",
       "2226         0  \n",
       "\n",
       "[168 rows x 2 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 200\n",
    "\n",
    "searchfor = [\"team lead\",\"store manager\",\"people lead\", \"return work\"]\n",
    "df[df['text'].str.contains('|'.join(searchfor))][['text','analysis']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    0.541667\n",
       "-1    0.369048\n",
       " 0    0.089286\n",
       "Name: analysis, dtype: float64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['text'].str.contains('|'.join(searchfor))]['analysis'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text        didn receive myshare bonus wondering anyone can help with advice maybe manager people lead could help got two point and half point the other day that wa incorrectly given call off for vaccine side...\n",
       "analysis                                                                                                                                                                                                         -1\n",
       "Name: 1420, dtype: object"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[1420][['text','analysis']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion & Recommendations:\n",
    "\n",
    "Using the Logistic Regression and TF-IDF Vectorizer as our production classification model, the 85% accuracy and AUC score of 0.93 suggests that most of the posts are accurately classified into its respective subreddits.\n",
    "\n",
    "More Negative sendiments on Walmart's subreddit posts are double compared to Costco.\n",
    "- For the sentiments of subreddit post of walmart, 19% are neutral, 34% negative & 47% positive.\n",
    "- For the sentiments of subreddit post of costco, 22% are neutral, 23% negative & 54% positive.\n",
    "\n",
    "We would conclude that Walmart's social media image on reddit is not ideal given the numerous posts by Walmart's employees complaining about their workplace environment and conditions. Also supported by the sentiment analysis in which we seek to classify text as having positive or negative emotion.\n",
    "\n",
    "Costco's social media image on the other hand fared well. Costco seems to be able to offer products and services that customers are interested in and had gone on reddit to discuss and give praises about it.\n",
    "\n",
    "**Recommendations**\n",
    "\n",
    "* To the __Primary stakeholder: Walmart Corporate-Empoyees__ (Because happy employees make happy customers, leading to positive improvements to Walmart’s brand image)\n",
    "\n",
    "    - Provide more benefits programmes：\n",
    "        - Have better employee benefits to attract and retain employees\n",
    "    - Suggest for Management Uplift：\n",
    "        - Consider improving overall management strategy to develop a better company culture\n",
    "    - Improve HR Policies：\n",
    "        - Uplift Annual leave and PTO policies\n",
    "        - Capitalise on Whistle Blowing policy to protect employees, for example, Human Resource Team can reinforce the fact that associates are able to report a concern online without their identities being exposed, especially if they have Team Leads and/or Store Managers who are not carrying out the necessary duties\n",
    "        - To start being pro-active in addressing and taking in feedback of their employees\n",
    "    - Improve SOP: \n",
    "        - Clear guidelines and protocols on how everything should be carried out in Walmart should be communicated clearly by employers to employees\n",
    "\n",
    "* To the __Secondary Stakeholder: Walmart Consumers__(Because happy customers keep us going everyday)\n",
    "\n",
    "    - Promotions：\n",
    "        - To have better cash rebate scheme and offer better deals, more sales to attract customers\n",
    "    - House Brands：\n",
    "        - To develop a House Brand similar to Costco's Kirkland Signature to increase brand loyalty\n",
    "    - Improve HR Policies：\n",
    "        - Uplift Annual leave and PTO policies. Capitalise on Whistle Blowing policy to protect employees, for example, Human Resource Team can reinforce the fact that associates are able to report a concern online without their identities being exposed, especially if they have Team Leads and/or Store Managers who are not carrying out the necessary duties\n",
    "    - Inhouse Foodcout/Food Stations: \n",
    "        - Setup a \"happy\" shopping with \"eating\" enviroment, potentially develop \"hot food\" stations or food courts that consumers can enjoy the full experience of shopping in Walmart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Future Steps\n",
    "\n",
    "- In the future, a larger dataset could be gathered. Currently, only around 1490 unique posts were gathered for Costco and 1497 posts for Walmart. A larger dataset would be better for us to make inference of the population. \n",
    "- More models could be applied, for example models such as artificial neural network could be explored in the future. The advanced models which might provide better predicitions. \n",
    "- We are now is comparing Costco Posts, We could compare more subreddit posts of supermarkets in USA to Walmart (Eg. Trader Joe's, Aldi etc.) \n",
    "- Our model is limited to just look into the title & selftext of each subreddit post, we can also look into the comments and upvotes in each post."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
